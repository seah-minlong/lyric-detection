{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a8f7b4",
   "metadata": {},
   "source": [
    "# Whisper Fine-tuning for Lyric Transcription\n",
    "\n",
    "Fine-tuning Whisper on my song collection to improve lyric transcription accuracy. Using Demucs for vocal separation then training on the isolated vocals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3bee2",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "Need audio files in `data/songs/` and matching lyrics in `data/lyrics/`. Filenames must match (e.g., `song1.mp3` → `song1.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets accelerate\n",
    "!pip install -q torch torchaudio\n",
    "!pip install -q demucs\n",
    "!pip install -q evaluate jiwer\n",
    "\n",
    "print(\"Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from datasets import Dataset, Audio\n",
    "import evaluate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711ed76c",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ecaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_vocals_demucs(input_song_path: str, output_dir: str = \"temp_vocals\") -> str:\n",
    "    \"\"\"Extract vocals using Demucs\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Processing: {os.path.basename(input_song_path)}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            \"python\", \"-m\", \"demucs\", \n",
    "            \"--two-stems\", \"vocals\",\n",
    "            \"-o\", output_dir, \n",
    "            input_song_path\n",
    "        ], capture_output=True, text=True, check=True)\n",
    "        \n",
    "        song_name = os.path.splitext(os.path.basename(input_song_path))[0]\n",
    "        vocal_path = os.path.join(output_dir, \"htdemucs\", song_name, \"vocals.wav\")\n",
    "        \n",
    "        if os.path.exists(vocal_path):\n",
    "            return vocal_path\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Vocal track not found at {vocal_path}\")\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Demucs error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Clean up lyrics text and remove structural markers\"\"\"\n",
    "    # Remove structural markers like [Intro], [Verse 1], [Chorus], etc.\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    # Remove extra whitespace (including from marker removal)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44acdc",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "DATA_DIR = \"data\"\n",
    "SONGS_DIR = os.path.join(DATA_DIR, \"songs\")\n",
    "LYRICS_DIR = os.path.join(DATA_DIR, \"lyrics\")\n",
    "VOCALS_DIR = \"processed_vocals\"\n",
    "\n",
    "os.makedirs(VOCALS_DIR, exist_ok=True)\n",
    "\n",
    "# Check directories exist\n",
    "if not os.path.exists(SONGS_DIR):\n",
    "    raise FileNotFoundError(f\"Songs directory not found: {SONGS_DIR}\")\n",
    "if not os.path.exists(LYRICS_DIR):\n",
    "    raise FileNotFoundError(f\"Lyrics directory not found: {LYRICS_DIR}\")\n",
    "\n",
    "print(f\"Songs: {SONGS_DIR}\")\n",
    "print(f\"Lyrics: {LYRICS_DIR}\")\n",
    "print(f\"Output: {VOCALS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb97778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    \"\"\"Process songs and match with lyrics\"\"\"\n",
    "    dataset_records = []\n",
    "\n",
    "    song_files = [f for f in os.listdir(SONGS_DIR) if f.endswith((\".mp3\", \".wav\", \".m4a\"))]\n",
    "    print(f\"Found {len(song_files)} songs\")\n",
    "\n",
    "    for song_file in tqdm(song_files, desc=\"Processing\"):\n",
    "        try:\n",
    "            base_name = os.path.splitext(song_file)[0]\n",
    "            lyrics_file = f\"{base_name}.txt\"\n",
    "            lyrics_path = os.path.join(LYRICS_DIR, lyrics_file)\n",
    "\n",
    "            if not os.path.exists(lyrics_path):\n",
    "                print(f\"Skipping {song_file}: no lyrics file\")\n",
    "                continue\n",
    "\n",
    "            # Load lyrics\n",
    "            with open(lyrics_path, 'r', encoding='utf-8') as f:\n",
    "                lyrics = f.read()\n",
    "\n",
    "            lyrics = normalize_text(lyrics)\n",
    "\n",
    "            if not lyrics:\n",
    "                print(f\"Skipping {song_file}: empty lyrics\")\n",
    "                continue\n",
    "\n",
    "            # Separate vocals\n",
    "            song_path = os.path.join(SONGS_DIR, song_file)\n",
    "            vocal_path = separate_vocals_demucs(song_path, VOCALS_DIR)\n",
    "\n",
    "            # Store path only (HF Audio will load it)\n",
    "            record = {\n",
    "                'audio': vocal_path,\n",
    "                'text': lyrics,\n",
    "                'song_name': base_name\n",
    "            }\n",
    "\n",
    "            dataset_records.append(record)\n",
    "            print(f\"✓ {song_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {song_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Processed {len(dataset_records)} songs\")\n",
    "    return dataset_records\n",
    "\n",
    "dataset_records = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa12cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HF dataset\n",
    "if len(dataset_records) == 0:\n",
    "    raise ValueError(\"No valid song-lyrics pairs found\")\n",
    "\n",
    "dataset = Dataset.from_list(dataset_records)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000, decode=False))\n",
    "\n",
    "print(f\"Dataset: {len(dataset)} examples\")\n",
    "print(f\"Features: {dataset.features}\")\n",
    "\n",
    "# Preview\n",
    "if len(dataset) > 0:\n",
    "    sample = dataset[0]\n",
    "    print(f\"\\nSample:\")\n",
    "    print(f\"  Song: {sample['song_name']}\")\n",
    "    print(f\"  Text: {len(sample['text'])} chars\")\n",
    "    print(f\"  Preview: {sample['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d09760",
   "metadata": {},
   "source": [
    "## Load Whisper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using small model - good balance of performance/resources\n",
    "MODEL_NAME = \"openai/whisper-small\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}\")\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # Get audio path\n",
    "    audio_path = batch[\"audio\"][\"path\"] if isinstance(batch[\"audio\"], dict) else batch[\"audio\"]\n",
    "\n",
    "    # Load audio using torchaudio\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Resample to 16kHz if needed\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to numpy for feature extractor\n",
    "    audio_array = waveform.squeeze().numpy()\n",
    "\n",
    "    # Extract input features using processor\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio_array,\n",
    "        sampling_rate=16000\n",
    "    ).input_features[0]\n",
    "\n",
    "    # Tokenize text using processor\n",
    "    tokenized = processor.tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=448,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=False\n",
    "    )\n",
    "    labels = tokenized.input_ids\n",
    "    if len(labels) == 0:\n",
    "        labels = [processor.tokenizer.pad_token_id]\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch\n",
    "\n",
    "print(\"Preprocessing dataset...\")\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names, desc=\"Processing\")\n",
    "\n",
    "print(\"Dataset preprocessed\")\n",
    "print(f\"Features: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483d7af",
   "metadata": {},
   "source": [
    "## Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset (80/20)\n",
    "if len(dataset) > 1:\n",
    "    train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = train_test_split[\"train\"]\n",
    "    eval_dataset = train_test_split[\"test\"]\n",
    "else:\n",
    "    # Only one song - use for both (not ideal but works)\n",
    "    train_dataset = dataset\n",
    "    eval_dataset = dataset\n",
    "    print(\"Warning: Only one song, using for both train/val\")\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(eval_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c842b",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d390a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator for speech-to-text tasks.\n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad input features\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Pad labels\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding token id's of the labels by -100\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "print(\"✅ Data collator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceffb73",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track WER during training\n",
    "metric_wer = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    \n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print(\"Metrics ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47134a23",
   "metadata": {},
   "source": [
    "## Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ddf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings - reduce batch size if OOM\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-lyrics-model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"LR: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58350c2",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6febd8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5dba82",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8015485",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb980d81",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"./whisper-lyrics-final\"\n",
    "\n",
    "trainer.save_model(model_save_path)\n",
    "processor.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "\n",
    "# Create zip for download\n",
    "!zip -r whisper-lyrics-model.zip whisper-lyrics-final/\n",
    "print(\"Backup created: whisper-lyrics-model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e66f6",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db728704",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training complete!\")\n",
    "print(f\"Dataset: {len(dataset)} songs\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Saved: {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
