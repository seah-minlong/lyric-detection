{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a8f7b4",
   "metadata": {},
   "source": [
    "# Train Whisper on English Songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3bee2",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Required folders (relative to notebook):\n",
    "- `data/songs/` : audio files (.mp3)\n",
    "- `data/lyrics/` : matching text files (`<basename>.txt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate\n",
    "!pip install -q torch torchaudio\n",
    "!pip install -q demucs\n",
    "!pip install -q librosa soundfile\n",
    "!pip install -q evaluate jiwer\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Transformers and datasets\n",
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperFeatureExtractor,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "from datasets import Dataset, Audio\n",
    "import evaluate\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711ed76c",
   "metadata": {},
   "source": [
    "## 2. Data Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ecaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_vocals_demucs(input_song_path: str, output_dir: str = \"temp_vocals\") -> str:\n",
    "    \"\"\"\n",
    "    Separate vocals from a song using Demucs.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"ğŸµ Separating vocals from: {os.path.basename(input_song_path)}\")\n",
    "    \n",
    "    try:\n",
    "        # Run Demucs vocal separation\n",
    "        result = subprocess.run([\n",
    "            \"python\", \"-m\", \"demucs\", \n",
    "            \"--two-stems\", \"vocals\",\n",
    "            \"-o\", output_dir, \n",
    "            input_song_path\n",
    "        ], capture_output=True, text=True, check=True)\n",
    "        \n",
    "        # Find the vocal track\n",
    "        song_name = os.path.splitext(os.path.basename(input_song_path))[0]\n",
    "        vocal_path = os.path.join(output_dir, \"htdemucs\", song_name, \"vocals.wav\")\n",
    "        \n",
    "        if os.path.exists(vocal_path):\n",
    "            return vocal_path\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Vocal track not found at {vocal_path}\")\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ Error in Demucs separation: {e}\")\n",
    "        print(f\"Stderr: {e.stderr}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_and_preprocess_audio(audio_path: str, target_sample_rate: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load and preprocess audio file for Whisper.\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_path, sr=target_sample_rate)\n",
    "    \n",
    "    # Ensure audio is mono\n",
    "    if audio.ndim > 1:\n",
    "        audio = librosa.to_mono(audio)\n",
    "    \n",
    "    return audio\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text for training.\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44acdc",
   "metadata": {},
   "source": [
    "## 3. Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your data paths\n",
    "DATA_DIR = \"data\"  # Update this if your data folder is in a different location\n",
    "SONGS_DIR = os.path.join(DATA_DIR, \"songs\")\n",
    "LYRICS_DIR = os.path.join(DATA_DIR, \"lyrics\")\n",
    "VOCALS_DIR = \"processed_vocals\"  # Where we'll store separated vocals\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(VOCALS_DIR, exist_ok=True)\n",
    "\n",
    "# Verify data directories exist\n",
    "if not os.path.exists(SONGS_DIR):\n",
    "    raise FileNotFoundError(f\"Songs directory not found: {SONGS_DIR}\")\n",
    "if not os.path.exists(LYRICS_DIR):\n",
    "    raise FileNotFoundError(f\"Lyrics directory not found: {LYRICS_DIR}\")\n",
    "\n",
    "print(f\"âœ… Data directories found:\")\n",
    "print(f\"  Songs: {SONGS_DIR}\")\n",
    "print(f\"  Lyrics: {LYRICS_DIR}\")\n",
    "print(f\"  Output vocals: {VOCALS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb97778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    \"\"\"\n",
    "    Create dataset by processing songs and matching with lyrics.\n",
    "    Returns a list of dicts where 'audio' is just the path (so HF Audio feature can load it).\n",
    "    \"\"\"\n",
    "    dataset_records = []\n",
    "\n",
    "    # Get all song files\n",
    "    song_files = [f for f in os.listdir(SONGS_DIR) if f.endswith((\".mp3\", \".wav\", \".m4a\"))]\n",
    "    print(f\"Found {len(song_files)} song files\")\n",
    "\n",
    "    for song_file in tqdm(song_files, desc=\"Processing songs\"):\n",
    "        try:\n",
    "            # Get base name without extension\n",
    "            base_name = os.path.splitext(song_file)[0]\n",
    "\n",
    "            # Check if corresponding lyrics file exists\n",
    "            lyrics_file = f\"{base_name}.txt\"\n",
    "            lyrics_path = os.path.join(LYRICS_DIR, lyrics_file)\n",
    "\n",
    "            if not os.path.exists(lyrics_path):\n",
    "                print(f\"âš ï¸  Skipping {song_file}: No matching lyrics file found\")\n",
    "                continue\n",
    "\n",
    "            # Load lyrics\n",
    "            with open(lyrics_path, 'r', encoding='utf-8') as f:\n",
    "                lyrics = f.read()\n",
    "\n",
    "            lyrics = normalize_text(lyrics)\n",
    "\n",
    "            if not lyrics:\n",
    "                print(f\"âš ï¸  Skipping {song_file}: Empty lyrics file\")\n",
    "                continue\n",
    "\n",
    "            # Separate vocals using Demucs\n",
    "            song_path = os.path.join(SONGS_DIR, song_file)\n",
    "            vocal_path = separate_vocals_demucs(song_path, VOCALS_DIR)\n",
    "\n",
    "            # IMPORTANT CHANGE:\n",
    "            # Instead of embedding raw audio samples (which became Python lists when serialized),\n",
    "            # store only the file path. The datasets Audio feature will load & (re)sample it.\n",
    "            record = {\n",
    "                'audio': vocal_path,   # path string only\n",
    "                'text': lyrics,\n",
    "                'song_name': base_name\n",
    "            }\n",
    "\n",
    "            dataset_records.append(record)\n",
    "            print(f\"âœ… Processed: {song_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {song_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nğŸ“Š Successfully processed {len(dataset_records)} songs\")\n",
    "    return dataset_records\n",
    "\n",
    "# Create the dataset\n",
    "dataset_records = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa12cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Hugging Face dataset\n",
    "if len(dataset_records) == 0:\n",
    "    raise ValueError(\"No valid song-lyrics pairs found. Please check your data.\")\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(dataset_records)\n",
    "\n",
    "# Cast audio column to Audio feature with decode disabled to avoid torchcodec dependency\n",
    "# We'll load audio manually in the prepare_dataset function.\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000, decode=False))\n",
    "\n",
    "print(f\"ğŸ“ Dataset created with {len(dataset)} examples\")\n",
    "print(f\"Dataset features: {dataset.features}\")\n",
    "\n",
    "# Show a sample (only path available before manual load)\n",
    "if len(dataset) > 0:\n",
    "    sample = dataset[0]\n",
    "    print(f\"\\nğŸ“ Sample:\")\n",
    "    print(f\"  Song: {sample['song_name']}\")\n",
    "    print(f\"  Text length: {len(sample['text'])} characters\")\n",
    "    print(f\"  Audio path: {sample['audio']['path'] if isinstance(sample['audio'], dict) else sample['audio']}\")\n",
    "    print(f\"  Text preview: {sample['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d09760",
   "metadata": {},
   "source": [
    "## 4. Model Setup and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Whisper model and processor\n",
    "MODEL_NAME = \"openai/whisper-small\"  # You can change this to base, medium, or large\n",
    "\n",
    "print(f\"ğŸ¤– Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load processor and model\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME)\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"âœ… Model loaded successfully\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ade31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function \n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # Get audio path\n",
    "    audio_path = batch[\"audio\"][\"path\"] if isinstance(batch[\"audio\"], dict) else batch[\"audio\"]\n",
    "\n",
    "    # Load audio (mono, 16 kHz)\n",
    "    audio_array, _ = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    # Extract input features\n",
    "    batch[\"input_features\"] = feature_extractor(\n",
    "        audio_array,\n",
    "        sampling_rate=16000\n",
    "    ).input_features[0]\n",
    "\n",
    "    # Tokenize text -> labels\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "print(\"ğŸ”„ Preprocessing dataset (simple)...\")\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Preprocessing\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Dataset preprocessed\")\n",
    "print(f\"Features: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483d7af",
   "metadata": {},
   "source": [
    "## 5. Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "if len(dataset) > 1:\n",
    "    # If we have multiple songs, create train/validation split\n",
    "    train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = train_test_split[\"train\"]\n",
    "    eval_dataset = train_test_split[\"test\"]\n",
    "else:\n",
    "    # If we only have one song, use it for both training and validation\n",
    "    train_dataset = dataset\n",
    "    eval_dataset = dataset\n",
    "    print(\"âš ï¸  Only one song found. Using the same data for training and validation.\")\n",
    "\n",
    "print(f\"ğŸ“Š Dataset split:\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c842b",
   "metadata": {},
   "source": [
    "## 6. Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d390a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator for speech-to-text tasks.\n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad input features\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Pad labels\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding token id's of the labels by -100\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "print(\"âœ… Data collator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceffb73",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "metric_wer = evaluate.load(\"wer\")\n",
    "metric_bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics.\n",
    "    \"\"\"\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    \n",
    "    # Replace -100 with pad token id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute WER\n",
    "    wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    # Compute BLEU\n",
    "    bleu = metric_bleu.compute(\n",
    "        predictions=pred_str, \n",
    "        references=[[ref] for ref in label_str]\n",
    "    )[\"bleu\"]\n",
    "    \n",
    "    return {\n",
    "        \"wer\": wer,\n",
    "        \"bleu\": bleu\n",
    "    }\n",
    "\n",
    "print(\"âœ… Metrics initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47134a23",
   "metadata": {},
   "source": [
    "## 8. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ddf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-lyrics-model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“‹ Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Output directory: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58350c2",
   "metadata": {},
   "source": [
    "## 9. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6febd8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5dba82",
   "metadata": {},
   "source": [
    "## 10. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8015485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"ğŸš€ Starting training...\")\n",
    "print(\"This may take a while depending on your dataset size and hardware.\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"âœ… Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb980d81",
   "metadata": {},
   "source": [
    "## 11. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model_save_path = \"./whisper-lyrics-final\"\n",
    "\n",
    "trainer.save_model(model_save_path)\n",
    "processor.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"ğŸ’¾ Model saved to: {model_save_path}\")\n",
    "\n",
    "# Create a zip file for easy download\n",
    "!zip -r whisper-lyrics-model.zip whisper-lyrics-final/\n",
    "print(\"ğŸ“¦ Model packaged as whisper-lyrics-model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a82b7b",
   "metadata": {},
   "source": [
    "## 12. Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bcf859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for testing\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create ASR pipeline with your trained model\n",
    "trained_asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_save_path,\n",
    "    tokenizer=model_save_path,\n",
    "    feature_extractor=model_save_path,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"âœ… Trained model loaded for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf2c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample from your dataset\n",
    "if len(eval_dataset) > 0:\n",
    "    # Get a test sample\n",
    "    test_sample = eval_dataset[0]\n",
    "    \n",
    "    # Reconstruct audio array for testing\n",
    "    audio_array = test_sample['input_features']\n",
    "    \n",
    "    # Get prediction from trained model\n",
    "    print(\"ğŸ¤ Testing trained model...\")\n",
    "    \n",
    "    # Note: We need to convert input_features back to audio array for the pipeline\n",
    "    # This is a simplified approach - in practice, you'd keep the original audio\n",
    "    \n",
    "    print(\"ğŸ“ Model is ready for testing!\")\n",
    "    print(\"To test with new audio files, use the trained_asr pipeline with audio files.\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  No evaluation data available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1837c60",
   "metadata": {},
   "source": [
    "## 13. Model Usage Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb26e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for using the trained model\n",
    "usage_code = '''\n",
    "# How to use your trained model:\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load your trained model\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"./whisper-lyrics-final\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Use with Demucs vocal separation (same as your original code)\n",
    "def transcribe_with_trained_model(song_path):\n",
    "    # 1. Separate vocals with Demucs\n",
    "    vocal_path = separate_vocals_demucs(song_path)\n",
    "    \n",
    "    # 2. Transcribe with your trained model\n",
    "    result = asr_pipeline(\n",
    "        vocal_path, \n",
    "        return_timestamps=True,\n",
    "        generate_kwargs={\"language\": \"en\"}\n",
    "    )\n",
    "    \n",
    "    return result[\"text\"]\n",
    "\n",
    "# Example usage\n",
    "lyrics = transcribe_with_trained_model(\"path/to/your/song.mp3\")\n",
    "print(lyrics)\n",
    "'''\n",
    "\n",
    "print(\"ğŸ“‹ Usage Instructions:\")\n",
    "print(usage_code)\n",
    "\n",
    "# Save usage instructions to file\n",
    "with open(\"model_usage.py\", \"w\") as f:\n",
    "    f.write(usage_code)\n",
    "\n",
    "print(\"ğŸ’¾ Usage instructions saved to model_usage.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e66f6",
   "metadata": {},
   "source": [
    "## 14. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db728704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training summary\n",
    "print(\"ğŸ‰ Training Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“Š Dataset size: {len(dataset)} songs\")\n",
    "print(f\"ğŸ¤– Base model: {MODEL_NAME}\")\n",
    "print(f\"ğŸ’¾ Saved model: {model_save_path}\")\n",
    "print(f\"ğŸ“¦ Zip file: whisper-lyrics-model.zip\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nğŸ“‹ Next Steps:\")\n",
    "print(\"1. Download whisper-lyrics-model.zip\")\n",
    "print(\"2. Extract the model files\")\n",
    "print(\"3. Use the model with your original pipeline\")\n",
    "print(\"4. Test on new songs to evaluate performance\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Tips for better results:\")\n",
    "print(\"- Use more training data for better performance\")\n",
    "print(\"- Ensure lyrics are high quality and properly formatted\")\n",
    "print(\"- Consider using a larger base model (medium/large) if you have GPU memory\")\n",
    "print(\"- Fine-tune hyperparameters based on your specific use case\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
